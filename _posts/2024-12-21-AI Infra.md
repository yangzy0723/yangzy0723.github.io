---
layout: mypost
title: AI Infra 概述
categories: [AI Infra]
---

# AI Infra 概述

AI Infra（AI 基础设施）是指为上层的AI算法应用提供支持的AI全栈底层技术，通过合理利用计算机体系结构，可以实现AI计算的加速和部署。

AI Infra主要包括以下内容：
- AI训练框架、推理引擎
- AI编译、计算架构
- AI硬件、体系结构

![ai_system](ai_system.png)

### AI Infra发展史

**2019-2021，黑铁时代**

17年那篇著名的《Attention is All you Need》论文发表，以Transformers为基础零件的不同模型结构接踵而至，Decoder-only的GPT-1（2018）、Encoder-only的Bert（2018），Encoder-Decoder的T5（2019）相继出现，开始在NLP领域大杀四方。互联网企业中的翻译、对话、推荐等应用场景相继被Transformers占领。

顺应潮流，有人开始研究变大Transformers模型。把模型变大在CNN时代是反直觉的，彼时大家正在通过NAS和AutoML等手段千方百计把模型变小，放到汽车、摄像头、手机里。Scaling Law的信徒主要是OpenAI和Google。在18年有用LSTM预测下一token的语言模型ELMo，OpenAI就用Transformers替换LSTM做了GPT-1，在Dota AI积累的哲学开始Scaling，随后就出了GPT-2和GPT-3。Google作为Transformers的策源地，17年就有提出了MoE架构的Transformers，20年已经能搞出用2K TPU训练的600B的大模型GShard，为此开发了基于TensorFlow的分布式训练框架MeshTensor。但和OpenAI不同，Google没有押宝Decoder-only的结构，而是大力发展Encoder-Decoder结构（也是Transformers最早被提出的结构）。DeepMind作为一个隶属Google但相对独立的英国研究机构，此时也有和GPT类似的Decoder结构模型Gopher和Chinchilla。

尽管，20年175B参数GPT-3的few-shot learining能力给业界带来了一些震撼，但国内对大模型技术路线持怀疑态度居多，拥趸寥寥，大部分算法同学对大模型并不感冒。观察原因来源于两方面。一方面，大多数人没有预训练的sense，当时NLP算法开发范式是搞私有数据+微调Bert，根据小模型时代的经验，应该专注在数据质量，而不是一个更大底座模型，哪怕先把模型变大两倍，也申请很多预算去训模型，ROI存疑。另一方面，Infra没有准备好。训练一个大模型可不是一般人可以玩得起的，现在训模型是算法+工程同学相互配合，那时候没有这种兵种搭配概念，业务团队里只有算法工程师，搞GPU对他们是头疼事情，尤其是上线一个需要两张GPU才能运行的模型，简直是灾难。而隶属于中台部门的Infra团队，又不了解大模型的风向，跨部门的信息是有壁的。

国内最有动力去Scale模型是做算法刷榜的团队，但大多刷榜的模型大多无法逃脱PR（宣传）后束之高阁的命运，不过这个过程锻炼了很多队伍，他们也成为后来ChatGPT爆火之后参与训练国内大模型的主力。

彼之砒霜，我之蜜糖。模型变大反而对AI Infra人是新机会。那个时代AI Infra的主旋律应用还是推荐系统，NVIDIA在押宝元宇宙作为新增长点，大模型对Infra同学也是新鲜事物。有OpenAI和Google前面开路，美国有些机构开始了将模型变大的探索性工作，主要是沿着把Encoder结构的Bert变大，不过对于训练来说Encoder和Decoder的差别不大。也正是借着这波机会，大模型训练框架Megatron-LM和DeepSpeed开始有了原型。20年微软搞了17B大Bert的Turing-NLG，其训练代码成为了DeepSpeed的原型。19年，NVIDIA搞了8.3B的Megatron-LM，没错Megatron-LM是一个大Bert的名字，Megatron-LM仓库里也是放训练模型的代码脚本，这些脚本实现了张量并行，后面逐渐发展成了最流行的训练框架。

这一时期，美国大厂训模型主要目的是试试水和秀肌肉，因而DeepSpeed和Megatron-LM最开始就是开源的，这也是一件好事。试想如果大模型出场就是核武器，大家都像22年之后OpenAI那样藏着掖着，Infra技术扩散必然没这么快了。

用大规模GPU训模型，小模型时代的数据并行无法胜任了。一些训练优化的基本概念开始定型，比如**ZeRO**，**张量并行**，**流水并行**，**Offloading**，**混合并行**等。这些技术之前也都有前身，比如18年工作混合并行的FlexFlow，流水并行来自给NAS出来的AmoebaNet训练的GPipe，ZeRO是一种Parameter Server特殊形式，等等。只不过这些技术在Transformers架构和高带宽互联网络中被更针对性地优化和适配了。

那个时候国内对大模型这种新鲜事物了解有限。袁老师的Oneflow是非常早就做大模型分布式训练的尝试的，SBP（张量并行策略）方式做自动化的模型并行理念还是非常超前的，可惜当时国内没有业务需求来支撑他们做PMF（产品与市场匹配）。华为的MindSpore也很早做自动并行的尝试。行动比较早团队的还有阿里的M6，智源的GLM和华为的Pangu-alpha。

除了少数大模型Believer，国内大厂对大模型不感冒，各大厂的云/中台团队刚刚完成一轮X minutes训练ImageNet的军备竞赛，还在思索用那么大规模的GPU去训练一个模型有什么商业价值。有一些NLP技术创业公司，技术路线还是是给不同的业务做定制的微调Bert。一些机构的投资人会打电话咨询技术人员，大模型是否是骗局。总体来说，22年之前，大模型对国内来说还是太超前，以至于共识非常薄弱，因此我称之为大模型Infra黑铁时代。

**2022-2023，黄金年代**

经历了黑铁年代储备期之后，以Meta开源OPT-175B模型为标志，22年开始大模型Infra开始迎来黄金年代。

一方面，伴随NVIDIA芯片的计划迭代，一台DGX SuperPOD（2021）已经可以很轻松搞定百亿参数的大模型训练了，NVIDIA为了推这种超级计算机，也把大模型列为一种杀手应用来宣传。除了大模型还真不知道什么应用能把它填满，21年11月腾讯弄了一台SuperPod样机，拉全公司各种Infra团队一起测试两个月，本来列了一个密集测试排期表，最后发现只有我们调PatrickStar的在用机器，后面我们直接独占了全机。

另一方面，有人开始看到大模型的威力了，硅谷春江水暖鸭先知。22年5月份，Meta把OPT-175B的权重开源出来，它是为复现GPT3训练的，用的是PyTorch FairScale，虽然模型效果不敢恭维，但是真的是造福了广大大模型科研人员，做AI Infra的人也终于有一个真实模型可以做实验了。22年6月，HuggingFace也组织三十多个国家机构多国部队，开源出来Bloom-176B模型，用的DeepSpeed框架。

硅谷创业公司中，Character.ai和Anthropic.ai这时已经成立一段时间了，22年伊始，在全国同心抗击疫情之计，国外已经好几路人马在紧锣密鼓训练GPT-3级别的模型。印象比较深的是22年4月份，Transformers论文有两个作者离开Google成立了一个叫Adept.ai的大模型公司（如今刚刚卖身Amazon），用大模型帮人完成复杂任务，他们twitter的demo展示大模型能根据输入文字让Python画出一个柱状图表，看到之后非常不可思议，那应该是我的ChatGPT时刻，现在再看类似东西已是见怪不怪了。

直到2022年11月，ChatGPT强势出圈，引爆了大模型。这一事件，显著加速了模型变大有价值共识的形成。大模型团队快速聚集了大量人力、物力、财力。大模型Infra也迎来了跨越式发展。

大模发展，训练先行。在训练系统领域，Megatron-LM和DeepSpeed在黄金年代快速迭代。不过说实话，前ChatGPT时代这两个软件质量堪忧，我感觉是处于无架构师状态，疯狂缝合各种research idea，系统bug很多，而且用户接口很不友好。但是，大模型训练框架，先发优势很重要。大家选型时不是考虑好不好用，而是看这个框架之前训出过什么模型。毕竟训练一次，小则几十万，大则几千万美金的投资，稳妥最重要。后ChatGPT时代，由NVIDIA品牌和技术背书的Megatron-LM的优势开始滚雪球般变大，到现在基本被其商业版本Nemo一统江湖。

在技术创新上，也是遍地是黄金。**并行策略**和**算子优化**等方面很多机会，比如**Sequence Parallelism**、**Flash Attention**之类simple but effective的作品都获得巨大影响力。预训练之后，还有 **SFT**，**RLHF**的需求，也有**训推混合调度**和**S-LoRA**之类工作的诞生。

大模型Infra也深刻影响了上层算法的发展轨迹。比如，很长时间大家不敢增大context length，因为Attention计算中间QK^T矩阵的内存是序列的平方项开销，导致有一段时间**Linear Attention**，**Approximate Attention**用精度换长度的研究之风盛行。**Memory Efficient Attention工程优化**出现之后，最著名的是Flash Attention，直接把内存平方项干没了，大家又回到了老老实实用标准Attention的正轨。

在推理系统领域，大发展的出现比训练晚很多，主要发生在2023年之后。一方面，模型没训练出来，也就没有推理需求。另一方面，Decoder结构没有定于一尊之前，推理加速也没研究到正点上。之前大家都在关注怎么优化Encoder Transformers的推理。怎么做算子融合，怎么消除变长输入的Padding，比较出彩的有字节的Effective Transformers。在Bert时代，用的最多是FasterTransformers（FT），笔者19年的一个项目TurboTransformers正是对标FT。开发FT的是NVIDIA中国团队，（made in China），我是亲眼见证了它从NVIDIA DeepLearning Example一个小角落，逐渐发展壮大，独立成一个单独产品线的。

Bert时代系统优化可以复用到GPT的 Prefill阶段，但是还缺少Decoding阶段关键问题的解决方案。Encoder到Decoder的范式迁移，对训练变化很小，但对推理影响很大。从计算密集问题变成Prefill阶段计算密集，Decoding阶段访存密集的超级复杂的问题。在Bert时代的各种优化都没法用到Decoding阶段里。由于Decoding输出长度是不确定的，导致两个难以解决的关键问题：
- 一、如何动态打Batching，在输出token长度不确定时减少无效的padding计算
- 二、如何动态分配GPU显存给KVCache且没有内存碎片被浪费

推理虽然起步晚，但是发展速度要比训练快很多倍。因为，推理资源需求小，门槛低，大家都能参与进来，集思广益，汇聚广大人民群众的智慧，很多问题都会立刻暴露，然后立刻解决。2022年，OSDI论文**ORCA**提出了Continous Batching，解决了问题一。而就在距离今天exactly一年前的2023年6月，国内绝大多数大模型从业这都不知道Continous Batching。2023年的，SOSP论文**Paged Attention**解决了问题二。

开源社区发展迅猛，排除Accelerate，DeepSpeed-Inference这些只做计算不做调度的Library，开源领域最早的推理框架是huggingface的TGI（text-generation-inference），最初是给huggingface页面上面host的模型做推理。不过先发优势在推理框架领域失效，一部分原因是此框架用Rust写了调度部分，让大部分人没法参与其中，曲高和寡了。这时候国内其实已经出现了一些不错的开源推理框架，比如上海AI Lab的LMDelopyer。

真正的Game Changer是伯克利的**vLLM**，2023年6月开源出来，以其独创的Paged Attention技术一战成名。这时候刚好各种大模型也都训出了第一个版本，vLLM一下子满足了这波集中上线部署的需求。2023年9月份，NVIDIA推出了**TensorRT-LLM**，先是定向开源给企业内测，后面又对外开源，也分走了推理一大块蛋糕。2023年初NVIDIA才正式组织力量去发展Decoder模型推理框架，TensorRT-LLM缝合了TensorRT，Triton-server和FT三大王牌产品，足以见得推理需求的收敛其实也是最近一年才形成的。2023年下半年，还有小而美的国产推理框架**LightLLM**出现，它是纯python的，用**triton**实现cuda kernel，硅谷一些最新的paper也基于它开发。

有了推理框架，很多实验性工作可以摊开。**GPTQ**，**AWQ**等量化方法。**投机采样**，**Medusa**等增加Decoding阶段计算访存比，**FastGen**、**ChunkPrefill**等Batching调度策略，**DistServe**、**Splitewise**等分离式调度策略，更多NPU支持。

训练推理的需求一下子就起来了，吸引很多人才加入大模型Infra领域，大模型Infra领域迎来了一波繁荣，普通人只要学习能力强，就有机会上车，因此我称之为黄金时代。

**2024-？，白银时代**

2024年，尽管大模型百花齐放，但是生产资料向头部集中，从业者阶级固化加剧，大模型Infra进入白银时代。在经历2023年的FOMO（Fear of Missout）带来疯狂之后，大家开始冷静下来，一些人开始退场，一些人开始扩张。

在预训练领域，GPU资源开始向头部集中。创业公司剩下那么六七家，部分和云厂商抱团。大厂内部也只有一个钦定的团队收走全部GPU做预训练。这个是和小模型时代显著不同的，之前每个业务团队都可以训练自己的模型，都能自己管理一些GPU算力。就好比，原来每个省都自己有一支部队，现在国家只有中央军了。因此，对人才的需求比传统AI业务要少，但是想入行的人极具增多，用人门槛有极具升高。如果不是加入国内那十几个预训练团队，大部分人可能和预训练无缘了。

在微调和推理领域，机会也在收缩。分开源和闭源模型两个方面来看收缩原因。对闭源模型，微调和推理都是还是被预训练团队垄断的，因为几个亿烧出来的模型权重不能外流，只能客户拿数据进驻和被私有化部署。对开源模型，之前大家可能会认为，有了开源模型人人都可以做预训练下游的微调+部署流程。一个反直觉现象，尽管开源大模型数量在增多，能力在增强，但是微调和训练需求在减少。
- 第一，微调的难度其实非常高，没有训模型经验是调不出自己预期的效果的，所以RAG方式大行其道，这只需要调用大模型MaaS API即可。
- 第二，推理也非常卷，集成量化、调度、投机采样每一项技术的最佳实践难度不低。而且现在一些潮流分离式，混部等技术，对工程要求越来越高。一个小团队去搞推理部署反而干不过一些免费的开源MaaS的API，那个后面都有专业人士优化。

综上，大模型是和业务非常解耦的一项技术，更像是云厂商或者芯片。传统后台在线、离线系统，因为很多东西和业务有关，并不是标准件，因此没有做到最佳实践也有存在价值。对于大模型Infra，有开源框架作为一个水位线，the best or nothing，如果做不到最好就没有存在价值。因此，也可以参考芯片产业，资源会集中在少数巨头手中，大部分只能参与更下游的配套，比如RAG，Agent之类的。

### AI Infra初创公司

硅基流动：聚焦AI Infra，服务模型大模型应用，瞄准推理领域，从头搭建了一套独立于伯克利的vLLM和英伟达的TensorRT—LLM之外的推理框架—SiliconLLM

无问芯穹：在AI Infra上，着眼于软硬一体的整体解决方案，聚焦从算法到芯片、从芯片集群到模型、再从模型到应用的三阶段“M×N”中间层产品

公测Infini-AI大模型开发与服务云平台：
- 异构云管平台
    - 从多元异构算力入手，打破单一芯片品牌训练资源瓶颈，提高算力供给水平，降低模型部署成本
    - 7月WAIC大会上，无问芯穹针对**多芯片异构生态竖井**的难题发布了异构分布式混训平台，以适应多模型与多芯片的格局
- 一站式AI平台
- 大模型服务平台

### AI Infra的未来

伴随着AI应用的快速发展，AI Infra需要能够快速适应新的变化和需求，增强基础设施的可扩展性和灵活性，而未来谁能够为多样化的应用场景提供个性化的大模型一站式部署方案或许就能够在这场竞争中胜出。

作为大模型Infra从业者，白银时代需要的是苦练基本功。在2023年，有很多人是在用信息差体现自己价值，某件事我知你不知，你试还得花时间，很多人在极度激烈竞争中也原意为信息差知识付费。今年这种机会会大幅减少，大家比拼的就是真本领了，**是否能快速follow新技术，是否能独立搞定一个复杂大系统，是否有更大的技术视野和其他合作方对话的能力**，这要求不仅了解Infra还解一些算法、云计算的知识，总体来说**传统工程师素养**变得尤为重要。

关于AI Infra的未来，夏立雪这样形容：“打开水龙头前，我们不需要知道水是从哪条河里来的。同理，未来我们用各种AI应用时，也不会知道它调用了哪些基座模型，用到了哪种加速卡的算力——这就是最好的AI Native 基础设施。”


参考：https://zhuanlan.zhihu.com/p/708594043